{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjtom\\anaconda3\\envs\\styledna2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from model.model import pSp, condi\n",
    "from model.DNAnet import DNAnet,VAE\n",
    "from utils.utils import align_face, totensor\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n",
      "Loading encoder weights from ckpt!\n",
      "Loading decoder weights from pretrained!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): condi(\n",
       "    (style1): Mapper(\n",
       "      (style): Sequential(\n",
       "        (0): Linear(in_features=612, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (style2): Mapper(\n",
       "      (style): Sequential(\n",
       "        (0): Linear(in_features=612, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (style3): Mapper(\n",
       "      (style): Sequential(\n",
       "        (0): Linear(in_features=612, out_features=512, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): PReLU(num_parameters=1)\n",
       "        (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    \"dna\" : \"../saved_models\\\\vae_4600.pth\",\n",
    "    \"enc\" : \"../pretrained_model/enc_4_2.pth\",\n",
    "    \"gan\" : \"../pretrained_model/stylegan2-ffhq-config-f.pt\",\n",
    "    \"map\" : '../pretrained_model/condi_4_2.pth',\n",
    "    \"mom\" : \"result\\\\assets\\\\dad.png\",\n",
    "    \"dad\" : \"result\\\\assets\\\\dad.png\",\n",
    "    \"out\" : 'result/vae_child1'\n",
    "}\n",
    "\n",
    "if not os.path.exists(args[\"out\"]):\n",
    "    os.makedirs(args[\"out\"])\n",
    "    print(f\"saving to: {args['out']}\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available else \"cpu\"\n",
    "print(f\"device {device}\")\n",
    "# Load model weights\n",
    "DNA = VAE(512,device=device).to(device)\n",
    "DNA.load_state_dict(torch.load(args[\"dna\"]), strict=True)\n",
    "DNA.eval()\n",
    "\n",
    "net = nn.DataParallel(pSp(3, args[\"enc\"], args[\"gan\"])).to(device)\n",
    "net.eval()\n",
    "\n",
    "mapper = nn.DataParallel(condi()).to(device)\n",
    "mapper.load_state_dict(torch.load(args[\"map\"]), strict=True)\n",
    "mapper.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(args, net, DNA, mapper):\n",
    "    # Inference\n",
    "    mImg = align_face(args[\"mom\"]).convert('RGB')\n",
    "    dImg = align_face(args[\"dad\"]).convert('RGB')\n",
    "\n",
    "    testAge =  torch.ones((1,1)) * (args[\"age\"]) /100 \n",
    "    if args[\"gender\"] == 'male':\n",
    "        testGen = torch.ones(1, 1).to(device)\n",
    "    else:\n",
    "        testGen = torch.zeros(1, 1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mImg = totensor(mImg).unsqueeze(0).to(device)\n",
    "        dImg = totensor(dImg).unsqueeze(0).to(device)\n",
    "\n",
    "        pW = torch.cat([net.module.encoder(mImg), net.module.encoder(dImg)],1)\n",
    "        sW_hat = DNA(pW)['rec']\n",
    "        sW_hat_expand = sW_hat.repeat(18, 1, 1).permute(1, 0, 2)\n",
    "        sW_hat_delta = mapper(sW_hat_expand, testAge,  testGen)\n",
    "        sImg_hat = net(sW_hat_expand + sW_hat_delta)\n",
    "    out_file = f'{args[\"out\"]}/result_{args[\"gender\"]}_{args[\"age\"]}.png'\n",
    "    save_image((sImg_hat+1)/2, out_file, nrow = 1)\n",
    "    return out_file, sImg_hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: female\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "images = []\n",
    "for gender in [\"male\", \"female\"]:\n",
    "    print(\"Gender:\",gender)\n",
    "    args[\"gender\"] = gender\n",
    "    # for age in tqdm(range(5,105,10), desc = \"Age\", position=0):\n",
    "    #     args[\"age\"] = age\n",
    "    for i in tqdm(range(10)):\n",
    "        args[\"age\"] = 25\n",
    "        out_file, image = infer(args, net, DNA, mapper)\n",
    "        results.append(out_file)\n",
    "        images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "batch_images = torch.stack(images,dim=0).squeeze(1)\n",
    "print(batch_images.shape)\n",
    "out_path = f'{args[\"out\"]}/results_combined.png'\n",
    "save_image((batch_images+1)/2, out_path, nrow = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
